---
title: "ðŸ’» <i>R</i> Workshop ðŸ’»"
subtitle: "Data Science Workflow"
author: "Luca Valnegri"
output:
  xaringan::moon_reader:
    css: ["default", "metropolis", "metropolis-fonts", "https://datamaps.uk/assets/WeR/xari.css"]
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---
layout: true
background-size: 570px
background-position: -10% -40%
<div class="wer-header"> <img class="logo" src="https://datamaps.uk/assets/WeR/logo_white.png"/> </div>
<div class="wer-footer"> <span> &copy;2019 WeR meetup <br> <a href = "https://github.com/WeR-stats/workshop-introduction_to_R"> GitHub Repository </a> </span> </div>

---
background-image: url(wheel01.png)

# Problem Formulation

.pull-right-dswf[

- Understand the business question and its requirements

- Identify project objectives, scope, and measures for success

- Set deadlines and milestones

]

???

- The process stays the same regardless of the type of the problem that is trying to be solved
  
- Usually, the components required in each step changes depending on the problem

---
background-image: url(wheel02.png)

# Data Preparation

.pull-right-dswf[

- Data Planning

- Data Collection

- Data Sourcing

- Data Storage Design

]

???

- Nowadays, most companies have a tremendous amount of data, so it's essential to decide what kind of data is necessary for the project

- Often, internal data are coupled with data from external sources

- Last you must determine where and how data are going to be stored, how to gain access to it. 


---
background-image: url(wheel03.png)

# Data Processing

.pull-right-dswf[

- Data Manipulation
  
- Data Wrangling
  
- Data Imputation
  
- Anomaly Detection
  
- Feature Engineering

]

???


- Let data get to the point at which it's ready for exploration and modeling. 

- This step is the responsibility of both the data engineer and the data scientist. The data engineer does most of the processing beforehand by making the data accessible, clean, and ready for the data scientist to work with. However, most data scientists still need to do some amount of data processing afterwards, usually involving missing values, outliers, and feature engineering.

---
background-image: url(wheel04.png)

# Data Exploration

.pull-right-dswf[

- Data Summaries

- Data Display

- Data Visualization

]

???

-
- Allows for the investigation of what is happening in the data that is not obvious at a glance: uncover hidden patterns within the data, build questions about the data, reject/accept the original hypotheses, inform about trend or seasonality.
 This is the phase where you gain insights into what needs to be done, and try to identify which data elements, or features, drive the behaviour you want to model.

---
background-image: url(wheel05.png)

# Data Understanding

.pull-right-dswf[

- Data Modelling

- Data Mining

- Forecasting
  
- GeoStatistics
  
- Network Analysis
  
- ...

]

???

- This phase usually consist first ofinhoosing one among different techniques, and typically multiple cycle iterations are needed to unpickhe one to actually use

. - Once the "best" technique has been selected, another set of cycles is then needed to evaluate the results and define the correct set of parameters that allows to achieve (or approximate) the desired outcome.

- A common misconception is thinking that this is the only step associated with "data science". The reality is instead data scientists spend on it less than 10% of their time, while 50-80% of it is actually used to prepare data.

- Difference between Explanation (statistical modeling) and Prediction (data mining / machine learning)

- Modeling steps for Supervised Learning: 

- 1] Split data into Training and Testing set. The idea is to hold a subset of the data back and use it later to test the model's effectiveness.)

- 2] Generate Model/Algorithm

- 3] Test Model

- 4] Evaluate Model: compare the predictions against the actual values to see how well the model performed 
     ==> Confusion Matrix: accuracy, precision, recall
     ==> Mean squared error (MSE)

- Beware of Overfitting! that the model works well only with the data used to train it. This happens when too many data elements are used in model training. A sign of overtraining is when you get a high level of prediction accuracy, such as anything close to 100%. 

- Another common practice to improve model performance, and minimize the chancec to overfit, is cross-validation. This method splits the data into subsets of the full dataset to ensure you're not overfitting a model to one training set. 


---
background-image: url(wheel06.png)

# Data Presentation

.pull-right-dswf[

- Document

- Flat File (CSV, TSV, FWF)
  
- Spreadsheet
  
- Reporting System
  
- Dashboard
  
- Web Application

]

???


---
background-image: url(wheel07.png)

# Data Integration

.pull-right-dswf[

- Containerization
  
]

???

- This is the implementation step that's often left to the developers and devOps who "owns" the product(s).

- It's more and more common though that data scientists are involved, see the "dataOps".




---
background-image: url(wheel08.png)

# Data Monitoring

.pull-right-dswf[

- 

]

???


---
# Thank You!

 - [`r icon::fa("twitter")` @datamaps](https://twitter.com/datamaps)

 - [`r icon::fa("github")` lvalnegri](https://github.com/lvalnegri)
 
 - [`r icon::fa("linkedin")` Luca Valnegri](https://www.linkedin.com/in/lucavalnegri)

